---
title: "Advancing public health and data science using INLA - Day 1"
author: "Janet van Niekerk  (janet.vanNiekerk@kaust.edu.sa)"
date: "May 2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "/Users/vanniej/Documents/GitHub/INLA-CDC_2023")
library("INLA")
library("DiagrammeR")
library("car")
library("ggpubr")
library("spdep")
library("RColorBrewer")
library("spatstat")
library("sp")
library("latticeExtra")
library("gridExtra")
library("gstat")
library("raster")
library("ggplot2")
library("ggfortify")
library("survival")
library("joineR")
library("BayesSurvival")
library("icenReg")
library("nloptr")
library("boot")
```
## Bayesian statistics
Conceptually we can illustrate Bayesian statistics with the following diagram.
```{r Bayeschart, echo = FALSE, fig.cap = "Figure 3.1: Schematic presentation of Bayesian statistics", dev='png'}
grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = oval]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']


      # edge definitions with the node IDs
      tab1 -> tab3;
      tab2 -> tab3;
      tab3 -> tab4;
      }

      [1]: 'Information from the data y'
      [2]: 'Prior or external information about M'
      [3]: 'Bayes theorem'
      [4]: 'Posterior information about M'
      ", height = 200)
```

## INLA

Integrated Nested Laplace Approximations (INLA) is an approximate method to do posterior inference. It does not use sampling, but explicitly calculates the posterior distributions of the elements in the latent field and the hyperparameters.

It is very very fast and accurate. and for huge samples (like climate models using global data), it is one of very few frameworks that can be used.

The only assumption needed is that the statistical model is a Latent Gaussian Model (LGM).

# Latent Gaussian models
This is a fancy name for a hierarchical Bayesian model of the following form:
\begin{eqnarray*}
&&Y\sim  F(\mu,\pmb{\theta}_1) \text{any likelihood}\\
&&\mu = g(\eta)\text{, with } \eta = \pmb{X\beta}^\top + \sum_{j=1}^{k}f^{j}(\pmb{u}_j|\pmb{\theta}_2)\\
&&\{\pmb{\eta},\pmb{B}, \pmb{f}\}\sim \color{blue}{N(\pmb{0},\pmb{Q}^{-1})} \\
&&\pmb{\theta}=\{\pmb{\theta}_1,\pmb{\theta}_2\}\sim G(...) \text{any prior for the hyperparameters}
\end{eqnarray*}

Most statistical models are actually LGM's - spatial models, temporal models, spline models, GLM, GLZ, survival models, joint models etc.

## R-INLA
The INLA methodology has been implemented in an R package named *INLA* (hereafter known as R-INLA). The package is installed as follows
```{r INLA_install, eval = FALSE}
install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE)
```
and the package can be updated periodically as follows
```{r INLA_update, eval = FALSE}
inla.update(testing = TRUE)
```
The syntax for performing approximate Bayesian inference using INLA is to use the *inla* call with the formula, family, data and additional optional arguments as follows: 
```{r inla_syntax, eval = F}
result <- inla(formula, family, data)
```
The result is then an *inla object* for which we an use summary to retrieve some summary statistics of the posterior marginal distributions of the unknown parameters as
```{r inla_summary, eval = F}
summary(result)
```
The unknown model parameters in $\pmb M$ is grouped into *fixed* effects, *random* effects or *hyperparameters*. The default priors and possible likelihoods can be found with
```{r likelihoods, eval = F}
#Which likelihoods?
names(inla.models()$likelihood)
#What are the default priors?
inla.set.control.fixed.default()
#Details of the models
inla.doc("sn")
```
For each of these groups we can obtain the summary statistics or the marginal posterior density using
```{r inla_effects, eval = F}
result$summary.fixed
result$summary.random
result$summary.hyperpar
result$marginals.fixed
result$marginals.random
result$marginals.hyperpar
```
We can also obtain the inference for the linear predictors $\pmb\eta$ if we add the optional argument *control.predictor = list(compute = TRUE)* to the inla call as
```{r inla_lin_pred, eval = F}
result <- inla(formula, family, data,
               control.predictor = list(compute = TRUE))
result$summary.linear.predictor
result$marginal.linear.predictor
```
Some vignettes:
```{r eval = F}
browseVignettes(package = "INLA")
```
## Example 1 - Binary regression
Consider a dichotomous outcome (the outcome variable $y$ can be either $1$ or $0$). Suppose we have covariates $\pmb z$ that we want to regress on the outcome variable $y$. Since the support of the outcome variable is not the real line, we cannot assume a Gaussian distribution for the outcome variable like in general linear models, instead we need a generalized linear model formulated specifically for dichotomous outcomes. Two such models that are popular in practice are logistic regression models and probit models. The logistic regression model links the covariates to the outcome variable through a *logit* link function such that
\begin{equation}
P(Y=1) = \frac{\exp(\eta)}{1+\exp(\eta)}
\end{equation} 
whereas the probit model links the covariates to the outcome variable through the *Gaussian cumulative distribution link function*, $\Phi(.)$ such that 
\begin{equation}
P(Y=1) = \Phi(\eta).
\end{equation} 
The default link function in INLA is the logit link as can be seen in 
```{r binomial_doc, eval = F}
inla.doc("binomial")
```
We use the *melanoma* dataset in the *boot* library. The dataset contains information on the survival of patients with malignant melanoma after a tumor removal surgery. 
```{r Example 1.a}
data(melanoma, package = "boot")
summary(melanoma)
```
The *status* variable is the response in this case where $1$ denotes that the patient died of melanoma and $0$ indicates that the patient did not die of melanoma. 
```{r Example 1.b}
#Code into binary events (1 - died from melanoma, 
#                   0 - did not die from melanoma)
melanoma$status[melanoma$status==2] = 0
melanoma$status[melanoma$status==3] = 0
prop_died = sum(melanoma$status[melanoma$status==1])/nrow(melanoma)

ggplot(melanoma, aes(x = status)) + geom_bar() +
  labs(x = "I(died)", y = "Frequency")
```  

We see that `r prop_died*100` $\%$ died from malignant melanoma during the study period. The covariates we consider are *age, sex, year*(year of surgery), *thickness, ulcer*(if the removed tumor were ulcerated, believed to be an important prognostic factor). For interpretibility, we scale the continuous covariates to have a zero mean and unit variance, and we scale *year* to be the number of years since the first surgery,.
```{r Example 1.c}
#Scale the continuous covariates for the fixed effects
melanoma$age = scale(melanoma$age)
melanoma$thickness = scale(melanoma$thickness)
melanoma$year = melanoma$year - min(melanoma$year)
```
Now we are ready to fit the logistic regression model 
\begin{equation}
P(status=1) = \frac{\exp(\eta)}{1+\exp(\eta)}
\end{equation} 
where 
\begin{equation}
\eta = \beta_0 + \beta_{Age}age + \beta_{Sex=1}I(Sex=1) + \beta_{Year}year + \beta_{Thickness}thickness + \beta_{Ulcer=1}I(Ulcer=1)
\end{equation} 
```{r Example 1.d}
#INLA model
model3.1 <- inla(formula = status ~ 1 + age + sex + year + thickness + ulcer,
                 family = "binomial",
                 data = melanoma)
```
We can view a summary of the results from the model, available in the INLA object *model3.1*. 
```{r Example 1.e}
summary(model3.1)
```
We see from the $95/%$ credible intervals that only *year* and *ulcer* are statistically significant covariates in explaining the variation in the *status* variable. Since $\beta_{Year}\in$ (`r model3.1$summary.fixed$'0.025quant'[4]`; `r model3.1$summary.fixed$'0.975quant'[4]`), we can infer that earlier years of surgery increases the odds of dying from malignant melanoma, compared to later years. Also, an ulcerated tumor increases the odds of dying from malignant melanoma, since $\beta_{Ulcer=1}\in$ (`r model3.1$summary.fixed$'0.025quant'[6]`; `r model3.1$summary.fixed$'0.975quant'[6]`).  

We can also extract the posterior distributions of the fixed effects from the INLA object *model3.1*
```{r Example 1.f}
#posterior marginals of the model components
pAge <- ggplot(data.frame(model3.1$marginals.fixed$age), aes(x=x, y=y)) + geom_line() + 
  labs(x = expression(beta["Age"]), y = "Density") + 
  geom_vline(xintercept = 0, col = "blue")

pSex <- ggplot(data.frame(model3.1$marginals.fixed$sex ), aes(x=x, y=y)) + geom_line() + 
  labs(x = expression(beta["Sex = 1"]), y = "Density") + 
  geom_vline(xintercept = 0, col = "blue")

pYear <- ggplot(data.frame(model3.1$marginals.fixed$year ), aes(x=x, y=y)) + geom_line() + 
  labs(x = expression(beta["Year"]), y = "Density") + 
  geom_vline(xintercept = 0, col = "blue")

pThickness <- ggplot(data.frame(model3.1$marginals.fixed$thickness ), aes(x=x, y=y)) + geom_line() + 
  labs(x = expression(beta["Thickness"]), y = "Density") + 
  geom_vline(xintercept = 0, col = "blue")

pUlcer <- ggplot(data.frame(model3.1$marginals.fixed$ulcer ), aes(x=x, y=y)) + geom_line() + 
  labs(x = expression(beta["Ulcer = 1"]), y = "Density") + 
  geom_vline(xintercept = 0, col = "blue")

ggarrange(pAge, pSex, pYear, pThickness, pUlcer,
          nrow = 2, ncol = 3)

```

We can also calculate posterior inference for the odds ratio's:
```{r Ex1 OR}
OR_age <- inla.tmarginal(function(x) exp(x),
                        model3.1$marginals.fixed$age)
plot(OR_age, type = "l")
inla.zmarginal(OR_age)

OR_ulcer <- inla.tmarginal(function(x) exp(x),
                      model3.1$marginals.fixed$ulcer) 
plot(OR_ulcer, type = "l")
inla.zmarginal(OR_ulcer)

OR_year <- inla.tmarginal(function(x) exp(x),
                      model3.1$marginals.fixed$year ) 
plot(OR_year, type = "l")
inla.zmarginal(OR_year)


```



## Example 2 - GLM,GLZ with random effects
In this example we use the North Carolina Sudden Infant Death Syndrome dataset.
```{r Example 2.a}
data(nc.sids)
head(nc.sids)
hist(nc.sids$SID74)
summary(nc.sids)
```
We see that the number of deaths is a count variable and therefore propose a Poisson regression model, i.e. 
$$y\sim Poi(E\theta)$$
with $$\theta = \exp(\eta)$$ and $E$ is the expected count. In this way, $\theta$ indicates the risk. We need to calculate the expected count for each observation,
```{r Example 2.b}
p_hat <- sum(nc.sids$SID74) / sum(nc.sids$BIR74)
nc.sids$E74 <- p_hat * nc.sids$BIR74
```
We use the NWBIR as a covariate to investigate if it has any influence on the risk of SIDS. We consider the proportion instead of the count,
```{r Example 2.c}
nc.sids$nwp_hat74 <- nc.sids$NWBIR74 / nc.sids$BIR74
```
Now we can fit the Poisson regression model, 
```{r Example 2.d}
result1_sids <- inla(formula = SID74 ~ nwp_hat74, family = "poisson",
                     E = E74,
                     data = nc.sids,
control.predictor = list(compute = TRUE))
```
The posterior summaries of this model is
```{r Example 2.e}
summary(result1_sids)
```
We can also visually investigate how the fitted values from the model fits onto the observed data,
```{r Example 2.f}
plot(nc.sids$SID74)
points(nc.sids$E74*result1_sids$summary.fitted.values$mean , col = "blue", pch = 10)
```
When we look at the data we see that the observations are form different counties and from a public health viewpoint this can provide interesting insight. We add a random intercept for each county to the Poisson regression model, so that we know have a mixed effects model.
```{r Example 2.g}
result2_sids <- inla(formula = SID74 ~ nwp_hat74 + f(CNTY.ID, model = "iid", hyper = list(theta = list(param = c(2, 2), prior = "loggamma"))),
                     family = "poisson",
                     E = E74,
                     data = nc.sids,
control.predictor = list(compute = TRUE))
summary(result2_sids)
plot(seq(0, 10, by = 0.1),
     dgamma(seq(0, 10, by = 0.1),
            shape = 2,
            scale = 2), type = "l")

#What is the default prior for the iid model?
inla.models()$latent$iid$hyper$theta
```
The extra hyperparameter, the standard deviation of the random intercept has the following posterior,
```{r Example 2.h}
res2_sd_rint <- inla.tmarginal(function(x) sqrt(1/x), result2_sids$marginals.hyperpar$`Precision for CNTY.ID`)
plot(res2_sd_rint, type = "l", xlab = expression(sigma), ylab = expression(paste("f(", sigma ,"|y)")))
```
We can also visually investigate how the fitted values from the model fits onto the observed data,
```{r Example 2.i}
plot(nc.sids$SID74)
points(nc.sids$E74*result1_sids$summary.fitted.values$mean , col = "blue", pch = 10)
points(nc.sids$E74*result2_sids$summary.fitted.values$mean , col = "red", pch = 19, cex = 0.5)
```
Adding a random intercept adds an independent effect, although the counties are probably not independent in terms of socio-economic diversity. So we should rather include a structured random effect such that some "intercepts" are correlated with each other, conveying the dependence in space (Day 2 material)
When we compare the fit between models we can look at the marginal log-likelihoods, DIC or WAIC.
```{r Example 2.o}
result1_sids <- inla(formula = SID74 ~ nwp_hat74, family = "poisson",
                     E = E74,
                     data = nc.sids,
control.predictor = list(compute = TRUE),
control.compute   = list(dic = TRUE, waic = TRUE))

result2_sids <- inla(formula = SID74 ~ nwp_hat74 + f(CNTY.ID, model = "iid"),
                     family = "poisson",
                     E = E74,
                     data = nc.sids,
control.predictor = list(compute = TRUE),
control.compute   = list(dic = TRUE, waic = TRUE))

print(matrix(rbind(cbind(result1_sids$mlik[1], 
                         result1_sids$dic[1],
                         result1_sids$waic[1]),
            cbind(result2_sids$mlik[1], 
                         result2_sids$dic[1],
                         result2_sids$waic[1])), 
            nrow = 2, ncol = 3,
            dimnames = list(c("Model 1", "Model 2"),
                            c("Marginal log-likelihood", "DIC", "WAIC")
                            )))
```
We can also do cross-validation as a means of measuring prediction performance (more on Day 2).

## Example 3 - Temporal and spline models
We use the AirPassengers dataset that contains the monthly totals of international airline passengers from 1949 to 1960. 
```{r Example 3.a}
data(AirPassengers)
airp.data <- data.frame(airp = as.vector(AirPassengers),
                        month = as.factor(rep(1:12, 12)), 
                        year = as.factor(rep(1949:1960, each = 12)),
                        ID = 1:length(AirPassengers))
summary(airp.data)

plot(airp.data$airp, type = "l")
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers")
airp.data$l_airp <- log(airp.data$airp)
```
Since the response evolves over time and is clearly associated with time, we will fit various temporal models for illustration. Obviously, the seasonal model is the best choice.
```{r Example 3.b}
inla.doc("ar1")
airp.ar1 <- inla(formula = l_airp ~ -1 + year + f(ID,         model = "ar1"),
        family = "gaussian",
        data = airp.data,
        control.predictor = list(compute = TRUE),
        control.family = list(hyper = list(prec =            list(param = c(1, 0.2)))))
                 
summary(airp.ar1)
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers")
lines(airp.ar1$summary.fitted.values$mean , col = "blue")
```
#SKIP
```{r Example 3.c}
#inla.doc("ar1c")
Z_ar1c <- model.matrix(~ 0 + year, data = airp.data)
Q.beta.prior <- Diagonal(12, 0.001)
airp.ar1c <- inla(l_airp ~ -1 + f(ID, model = "ar1c",
                                  args.ar1c = list(Z = Z_ar1c, Q.beta = Q.beta.prior)),
                 control.family = list(hyper = list(prec = list(param = c(1, 0.2)))),
                 data = airp.data, control.predictor = list(compute = TRUE))
summary(airp.ar1c)
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers
     ")
lines(airp.ar1c$summary.fitted.values$mean, col = "green")
```

```{r Example 3.d}
inla.doc("seasonal")
airp.seasonal <- inla(formula = l_airp ~ -1 + year + f(ID, model = "seasonal", season.length = 12),
        family = "gaussian",
        data = airp.data,
        control.predictor = list(compute = TRUE),
        control.family = list(hyper = list(prec =            list(param = c(1, 0.2)))))
summary(airp.seasonal)
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers
     ")
lines(airp.seasonal$summary.fitted.values$mean , col = "red")
```
```{r Example 3.e}
inla.doc("rw")
airp.rw1 <- inla(formula = l_airp ~ -1 + year + f(ID, model = "rw1", scale.model = TRUE),
        family = "gaussian",
        data = airp.data,
        control.predictor = list(compute = TRUE),
        control.family = list(hyper = list(prec =            list(param = c(1, 0.2)))))
summary(airp.rw1)

airp.rw2 <- inla(formula = l_airp ~ -1 + year + f(ID, model = "rw2", scale.model = TRUE),
        family = "gaussian",
        data = airp.data,
        control.predictor = list(compute = TRUE),
        control.family = list(hyper = list(prec =            list(param = c(1, 0.2)))))
summary(airp.rw2)
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers
     ")
lines(airp.rw1$summary.fitted.values$mean , col = "purple")
lines(airp.rw2$summary.fitted.values$mean , col = "orange", lwd = 2)
```
Now let us consider the fit of all the models.
```{r Example 3.f}
plot(log(airp.data$airp), type = "l", xlab = "Year", ylab = "Log of the count of passengers
     ")
lines(airp.ar1$summary.fitted.values$mean, col = "blue")
#lines(airp.ar1c$summary.fitted.values$mean, col = "green")
lines(airp.seasonal$summary.fitted.values$mean, col = "red")
lines(airp.rw1$summary.fitted.values$mean, col = "purple")
lines(airp.rw2$summary.fitted.values$mean, col = "orange")
```

##Random walk 2
```{r}
x <- seq(-10,10, by = 0.01)
y = x^2

y <- rnorm(n = length(x), x^2, sd = 10)

plot(x, y)

model_s <- inla(formula = y ~ -1 + f(inla.group(x, n = 100), model = "rw2", scale.model = TRUE, constr = F), data = data.frame(x=x, y=y))

summary(model_s)
plot(x, y, type= "l")
lines(model_s$summary.random$`inla.group(x, n = 100)`$ID,
    model_s$summary.random$`inla.group(x, n = 100)`$mean, col = "blue", lwd = 2)

```



  